From 33a7b98fe33d5e51e7b2b2161d4d02057a91555a Mon Sep 17 00:00:00 2001
From: Kasin Li <donglil@codeaurora.org>
Date: Fri, 10 Jun 2016 17:12:43 -0700
Subject: [PATCH 103/105] drm: adreno: Support resource parsing for Adreno5

This resource including IOMMU, the clocks, and the name of firmwares.

Change-Id: Iad6a7cb0f97cc850ddb8013f4a543ee5e812ddcf
Signed-off-by: Kasin Li <donglil@codeaurora.org>
---
 drivers/gpu/drm/msm/adreno/adreno_device.c | 242 +++++++++++++++++++++++------
 drivers/gpu/drm/msm/msm_gpu.c              |  30 ++--
 drivers/gpu/drm/msm/msm_gpu.h              |  17 +-
 3 files changed, 232 insertions(+), 57 deletions(-)

diff --git a/drivers/gpu/drm/msm/adreno/adreno_device.c b/drivers/gpu/drm/msm/adreno/adreno_device.c
index 8b94c1d..cb3ec5d 100644
--- a/drivers/gpu/drm/msm/adreno/adreno_device.c
+++ b/drivers/gpu/drm/msm/adreno/adreno_device.c
@@ -17,6 +17,7 @@
  * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/of_platform.h>
 #include "adreno_gpu.h"
 
 #if defined(CONFIG_MSM_BUS_SCALING) && !defined(CONFIG_OF)
@@ -31,57 +32,32 @@ module_param_named(hang_debug, hang_debug, bool, 0600);
 
 struct msm_gpu *a3xx_gpu_init(struct drm_device *dev);
 struct msm_gpu *a4xx_gpu_init(struct drm_device *dev);
+struct msm_gpu *a5xx_gpu_init(struct drm_device *dev);
 
 static const struct adreno_info gpulist[] = {
 	{
-		.rev   = ADRENO_REV(3, 0, 5, ANY_ID),
-		.revn  = 305,
-		.name  = "A305",
-		.pm4fw = "a300_pm4.fw",
-		.pfpfw = "a300_pfp.fw",
-		.gmem  = SZ_256K,
-		.init  = a3xx_gpu_init,
-	}, {
-		.rev   = ADRENO_REV(3, 0, 6, 0),
-		.revn  = 307,        /* because a305c is revn==306 */
-		.name  = "A306",
-		.pm4fw = "a300_pm4.fw",
-		.pfpfw = "a300_pfp.fw",
-		.gmem  = SZ_128K,
-		.init  = a3xx_gpu_init,
-	}, {
-		.rev   = ADRENO_REV(3, 2, ANY_ID, ANY_ID),
-		.revn  = 320,
-		.name  = "A320",
-		.pm4fw = "a300_pm4.fw",
-		.pfpfw = "a300_pfp.fw",
-		.gmem  = SZ_512K,
-		.init  = a3xx_gpu_init,
-	}, {
-		.rev   = ADRENO_REV(3, 3, 0, ANY_ID),
-		.revn  = 330,
-		.name  = "A330",
-		.pm4fw = "a330_pm4.fw",
-		.pfpfw = "a330_pfp.fw",
+		.rev   = ADRENO_REV(5, 3, 0, 0),
+		.revn  = 530,
+		.name  = "A530",
+		.pm4fw = "a530v1_pm4.fw",
+		.pfpfw = "a530v1_pfp.fw",
 		.gmem  = SZ_1M,
-		.init  = a3xx_gpu_init,
+		.init  = a5xx_gpu_init,
 	}, {
-		.rev   = ADRENO_REV(4, 2, 0, ANY_ID),
-		.revn  = 420,
-		.name  = "A420",
-		.pm4fw = "a420_pm4.fw",
-		.pfpfw = "a420_pfp.fw",
-		.gmem  = (SZ_1M + SZ_512K),
-		.init  = a4xx_gpu_init,
+		.rev   = ADRENO_REV(5, 3, 0, ANY_ID),
+		.revn  = 530,
+		.name  = "A530",
+		.pm4fw = "a530_pm4.fw",
+		.pfpfw = "a530_pfp.fw",
+		.zap_name = "a530_zap",
+		.regfw_name = "a530v3_seq.fw2",
+		.gmem  = SZ_1M,
+		.init  = a5xx_gpu_init,
 	},
 };
 
-MODULE_FIRMWARE("a300_pm4.fw");
-MODULE_FIRMWARE("a300_pfp.fw");
-MODULE_FIRMWARE("a330_pm4.fw");
-MODULE_FIRMWARE("a330_pfp.fw");
-MODULE_FIRMWARE("a420_pm4.fw");
-MODULE_FIRMWARE("a420_pfp.fw");
+MODULE_FIRMWARE("a530_pm4.fw");
+MODULE_FIRMWARE("a530_pfp.fw");
 
 static inline bool _rev_match(uint8_t entry, uint8_t id)
 {
@@ -158,6 +134,184 @@ struct msm_gpu *adreno_load_gpu(struct drm_device *dev)
 	return gpu;
 }
 
+
+struct drma_iommu *get_gpu_iommu(struct platform_device *pdev)
+{
+	struct adreno_platform_config *platform_config;
+	platform_config = pdev->dev.platform_data;
+	return &platform_config->iommu;
+}
+
+void enable_iommu_clks(struct platform_device *pdev)
+{
+	int j;
+	struct drma_iommu *iommu = get_gpu_iommu(pdev);
+	for (j = 0; j < KGSL_IOMMU_MAX_CLKS; j++) {
+		if (iommu->clks[j])
+			clk_prepare_enable(iommu->clks[j]);
+	}
+}
+
+void disable_iommu_clks(struct platform_device *pdev)
+{
+	int j;
+	struct drma_iommu *iommu = get_gpu_iommu(pdev);
+
+	for (j = 0; j < KGSL_IOMMU_MAX_CLKS; j++) {
+		if (iommu->clks[j])
+			clk_disable_unprepare(iommu->clks[j]);
+	}
+}
+
+static const struct {
+	int id;
+	char *name;
+} kgsl_iommu_cbs[] = {
+	{ KGSL_IOMMU_CONTEXT_USER, "gfx3d_user", },
+	{ KGSL_IOMMU_CONTEXT_SECURE, "gfx3d_secure" },
+};
+
+static int _adreno_iommu_cb_probe(
+		struct drma_iommu *iommu, struct device_node *node)
+{
+	struct platform_device *pdev = of_find_device_by_node(node);
+	struct drma_iommu_context *ctx;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(kgsl_iommu_cbs); i++) {
+		if (!strcmp(node->name, kgsl_iommu_cbs[i].name)) {
+			int id = kgsl_iommu_cbs[i].id;
+
+			ctx = &iommu->ctx[id];
+			ctx->id = id;
+			ctx->cb_num = -1;
+			ctx->name = kgsl_iommu_cbs[i].name;
+
+			break;
+		}
+	}
+
+	if (ctx == NULL) {
+		pr_err("dt: Unknown context label %s\n", node->name);
+		return -EINVAL;
+	}
+
+	/* TODO set secure flag. */
+
+	/* this property won't be found for all context banks */
+	if (of_property_read_u32(node, "qcom,gpu-offset", &ctx->gpu_offset))
+		ctx->gpu_offset = UINT_MAX;
+
+	/* arm-smmu driver we'll have the right device pointer here. */
+	if (of_find_property(node, "iommus", NULL))
+		ctx->dev = &pdev->dev;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+static const struct {
+	char *feature;
+	int bit;
+} kgsl_iommu_features[] = {
+	{ "qcom,retention", KGSL_MMU_RETENTION },
+	{ "qcom,global_pt", KGSL_MMU_GLOBAL_PAGETABLE },
+	{ "qcom,hyp_secure_alloc", KGSL_MMU_HYP_SECURE_ALLOC },
+	{ "qcom,force-32bit", KGSL_MMU_FORCE_32BIT },
+	{ "qcom,coherent-htw", KGSL_MMU_COHERENT_HTW },
+};
+
+static int adreno_iommu_probe(struct platform_device *pdev)
+{
+	int i = 0;
+	struct device_node *node;
+	const char *cname;
+	struct property *prop;
+	u32 reg_val[2];
+	struct device_node *child;
+	struct adreno_platform_config *platform_config;
+	struct drma_iommu *iommu;
+	struct platform_device *smmupdev;
+
+	platform_config = pdev->dev.platform_data;
+	iommu = &platform_config->iommu;
+
+	node = of_find_compatible_node(pdev->dev.of_node,
+		NULL, "qcom,kgsl-smmu-v1");
+
+	if (node == NULL)
+		node = of_find_compatible_node(pdev->dev.of_node,
+			NULL, "qcom,kgsl-smmu-v2");
+
+	if (node == NULL)
+		return -ENODEV;
+
+	smmupdev = of_find_device_by_node(node);
+	BUG_ON(smmupdev == NULL);
+
+	if (of_device_is_compatible(node, "qcom,kgsl-smmu-v1"))
+		iommu->version = 1;
+	else
+		iommu->version = 2;
+
+	if (of_property_read_u32_array(node, "reg", reg_val, 2)) {
+		pr_err("dt: Unable to read KGSL IOMMU register range\n");
+		return -EINVAL;
+	}
+	iommu->regstart = reg_val[0];
+	iommu->regsize = reg_val[1];
+
+	/* Protecting the SMMU registers is mandatory */
+	if (of_property_read_u32_array(node, "qcom,protect", reg_val, 2)) {
+		pr_err("dt: no iommu protection range specified\n");
+		return -EINVAL;
+	}
+	iommu->protect_reg_base = reg_val[0] / sizeof(u32);
+	iommu->protect_reg_range = ilog2(reg_val[1] / sizeof(u32));
+
+	of_property_for_each_string(node, "clock-names", prop, cname) {
+		struct clk *c = devm_clk_get(&smmupdev->dev, cname);
+
+		if (IS_ERR(c)) {
+			pr_err("dt: Couldn't get clock: %s\n", cname);
+			return -ENODEV;
+		}
+		if (i >= KGSL_IOMMU_MAX_CLKS) {
+			pr_err("dt: too many clocks defined.\n");
+			return -EINVAL;
+		}
+
+		iommu->clks[i] = c;
+		++i;
+	}
+	enable_iommu_clks(pdev);
+
+	/* TODO: GPU features detects */
+
+	if (of_property_read_u32(node, "qcom,micro-mmu-control",
+		&iommu->micro_mmu_ctrl))
+		iommu->micro_mmu_ctrl = UINT_MAX;
+
+	/* TODO: qcom,secure_align_mask */
+
+	/* Fill out the rest of the devices in the node */
+	of_platform_populate(node, NULL, NULL, &pdev->dev);
+
+	for_each_child_of_node(node, child) {
+		int ret = 0;
+
+		if (!of_device_is_compatible(child, "qcom,smmu-kgsl-cb"))
+			continue;
+
+		ret = _adreno_iommu_cb_probe(iommu, child);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
 static void set_gpu_pdev(struct drm_device *dev,
 		struct platform_device *pdev)
 {
@@ -254,7 +408,7 @@ static int adreno_bind(struct device *dev, struct device *master, void *data)
 #endif
 	dev->platform_data = &config;
 	set_gpu_pdev(dev_get_drvdata(master), to_platform_device(dev));
-	return 0;
+	return adreno_iommu_probe(to_platform_device(dev));
 }
 
 static void adreno_unbind(struct device *dev, struct device *master,
diff --git a/drivers/gpu/drm/msm/msm_gpu.c b/drivers/gpu/drm/msm/msm_gpu.c
index e01009f..e4ccb05 100644
--- a/drivers/gpu/drm/msm/msm_gpu.c
+++ b/drivers/gpu/drm/msm/msm_gpu.c
@@ -615,20 +615,25 @@ int msm_gpu_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit,
 static irqreturn_t irq_handler(int irq, void *data)
 {
 	struct msm_gpu *gpu = data;
+
 	return gpu->funcs->irq(gpu);
 }
 
 static const char *clk_names[] = {
-		"src_clk", "core_clk", "iface_clk", "mem_clk", "mem_iface_clk",
-		"alt_mem_iface_clk",
+		"src_clk", "core_clk", "iface_clk", "rbbmtimer_clk",
+		"mem_clk", "mem_iface_clk", "alt_mem_iface_clk", "mx_clk"
 };
 
+#define RB_SIZE    SZ_32K
+
 int msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,
 		struct msm_gpu *gpu, const struct msm_gpu_funcs *funcs,
-		const char *name, const char *ioname, const char *irqname, int ringsz)
+		const char *name, const char *ioname, const char *irqname)
 {
-	struct iommu_domain *iommu;
+	struct iommu_domain *iommu_domain;
 	int i, ret;
+	struct drma_iommu *iommu = get_gpu_iommu(pdev);
+	struct device *iommu_dev = iommu->ctx[KGSL_IOMMU_CONTEXT_USER].dev;
 
 	if (WARN_ON(gpu->num_perfcntrs > ARRAY_SIZE(gpu->last_cntrs)))
 		gpu->num_perfcntrs = ARRAY_SIZE(gpu->last_cntrs);
@@ -704,27 +709,30 @@ int msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,
 	 * and have separate page tables per context.  For now, to keep things
 	 * simple and to get something working, just use a single address space:
 	 */
-	iommu = iommu_domain_alloc(&platform_bus_type);
-	if (iommu) {
+	iommu_domain = iommu_domain_alloc(&platform_bus_type);
+	if (!IS_ERR_OR_NULL(iommu_domain)) {
 		dev_info(drm->dev, "%s: using IOMMU\n", name);
-		gpu->mmu = msm_iommu_new(&pdev->dev, iommu);
+		gpu->mmu = msm_smmu_new(iommu_dev, MSM_SMMU_DOMAIN_GPU);
 		if (IS_ERR(gpu->mmu)) {
 			ret = PTR_ERR(gpu->mmu);
-			dev_err(drm->dev, "failed to init iommu: %d\n", ret);
+			dev_err(drm->dev,
+				"failed to init iommu domain: %d\n", ret);
 			gpu->mmu = NULL;
-			iommu_domain_free(iommu);
+			iommu_domain_free(iommu_domain);
 			goto fail;
 		}
 
 	} else {
-		dev_info(drm->dev, "%s: no IOMMU, fallback to VRAM carveout!\n", name);
+		gpu->mmu = NULL;
+		dev_info(drm->dev, "%s: no IOMMU, fallback to VRAM!\n",
+			 name);
 	}
 	gpu->id = msm_register_mmu(drm, gpu->mmu);
 
 
 	/* Create ringbuffer: */
 	mutex_lock(&drm->struct_mutex);
-	gpu->rb = msm_ringbuffer_new(gpu, ringsz);
+	gpu->rb = msm_ringbuffer_new(gpu, RB_SIZE);
 	mutex_unlock(&drm->struct_mutex);
 	if (IS_ERR(gpu->rb)) {
 		ret = PTR_ERR(gpu->rb);
diff --git a/drivers/gpu/drm/msm/msm_gpu.h b/drivers/gpu/drm/msm/msm_gpu.h
index 2accfc0..d6fd5c6 100644
--- a/drivers/gpu/drm/msm/msm_gpu.h
+++ b/drivers/gpu/drm/msm/msm_gpu.h
@@ -23,6 +23,7 @@
 
 #include "msm_drv.h"
 #include "msm_ringbuffer.h"
+#include "drma_iommu.h"
 
 struct msm_gem_submit;
 struct msm_gpu_perfcntr;
@@ -100,7 +101,7 @@ struct msm_gpu {
 
 	/* Power Control: */
 	struct regulator *gpu_reg, *gpu_cx;
-	struct clk *ebi1_clk, *grp_clks[6];
+	struct clk *ebi1_clk, *grp_clks[8];
 	uint32_t fast_rate, slow_rate, bus_freq;
 
 #ifdef CONFIG_MSM_BUS_SCALING
@@ -141,6 +142,17 @@ struct msm_gpu_perfcntr {
 	const char *name;
 };
 
+static inline void gpu_write_mask(struct msm_gpu *gpu,
+					unsigned int reg,
+					unsigned int mask, unsigned int bits)
+{
+	unsigned int val = 0;
+
+	val = msm_readl(gpu->mmio + (reg << 2));
+	val &= ~mask;
+	msm_writel(val | bits, gpu->mmio + (reg << 2));
+}
+
 static inline void gpu_write(struct msm_gpu *gpu, u32 reg, u32 data)
 {
 	msm_writel(data, gpu->mmio + (reg << 2));
@@ -165,9 +177,10 @@ int msm_gpu_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit,
 
 int msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,
 		struct msm_gpu *gpu, const struct msm_gpu_funcs *funcs,
-		const char *name, const char *ioname, const char *irqname, int ringsz);
+		const char *name, const char *ioname, const char *irqname);
 void msm_gpu_cleanup(struct msm_gpu *gpu);
 
+struct drma_iommu *get_gpu_iommu(struct platform_device *pdev);
 struct msm_gpu *adreno_load_gpu(struct drm_device *dev);
 void __init adreno_register(void);
 void __exit adreno_unregister(void);
-- 
1.9.1

